# YouTube RAG-Based Conversational System

## Project Overview
This project implements a **local Retrieval-Augmented Generation (RAG) system** to query YouTube video content using natural language. Users can ask questions about a video and receive **context-aware answers** generated by a large language model (Mistral LLM).  

The system combines **audio extraction**, **speech-to-text transcription**, **semantic search**, and **LLM-based answer generation**, all running locally, making it privacy-friendly and offline-capable.

---

## Features
- **Audio Extraction:** Download audio from YouTube videos using `yt-dlp`.  
- **Transcription:** Convert audio to text with **OpenAI Whisper**.  
- **Semantic Search:** Index transcripts using **ChromaDB** embeddings.  
- **Context-Aware Answers:** Generate answers using **Ollama‚Äôs Mistral LLM**.  
- **Context Isolation:** Ensures queries for one video do not affect others.  
- **Local Execution:** Fully local pipeline; no cloud API required.  

---

## Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/YouTube-RAG-Project.git
cd YouTube-RAG-Project
Create a virtual environment

bash
Copy code
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
Install dependencies

bash
Copy code
pip install -r requirements.txt
Usage
Run the main program with a YouTube video URL and a query:

bash
Copy code
python main.py --video_url "https://www.youtube.com/watch?v=XXXXXXX" --query "What is the main topic of this video?"
Example Output:

kotlin
Copy code
Question: What is the main topic of this video?
Answer: The video explains the basics of Retrieval-Augmented Generation and its applications in NLP tasks.
Project Structure
bash
Copy code
YouTube-RAG-Project/
‚îÇ
‚îú‚îÄ‚îÄ main.py              # Entry point for running the program
‚îú‚îÄ‚îÄ rag_system.py        # RAG workflow (retrieval + generation)
‚îú‚îÄ‚îÄ transcription.py     # Audio extraction and Whisper transcription
‚îú‚îÄ‚îÄ embeddings.py        # Embedding creation and ChromaDB storage
‚îú‚îÄ‚îÄ utils.py             # Helper functions
‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îî‚îÄ‚îÄ examples/
     ‚îú‚îÄ‚îÄ sample_video_url.txt
     ‚îî‚îÄ‚îÄ sample_query.txt
Dependencies
Python 3.10+

yt-dlp ‚Äì YouTube audio extraction

openai-whisper ‚Äì Speech-to-text transcription

chromadb ‚Äì Vector database for semantic search

ollama ‚Äì Local LLM inference

Other utilities: numpy, tqdm, pandas

Install all dependencies via:

bash
Copy code
pip install -r requirements.txt
How It Works
Extract Audio: Download video audio using yt-dlp.

Transcribe Audio: Convert audio to text using Whisper.

Embed Transcripts: Split transcripts into smaller chunks and generate embeddings using ChromaDB.

Query: Accept user question and retrieve relevant transcript chunks using semantic similarity.

Generate Answer: Provide the retrieved context to Mistral LLM to produce accurate, context-aware answers.

Display Result: Output the answer in the terminal (or Streamlit interface if added).

Example
Video URL:

arduino
Copy code
https://www.youtube.com/watch?v=dQw4w9WgXcQ
User Query:

css
Copy code
"What is the main topic of the video?"
Expected Output:

vbnet
Copy code
Answer: The video explains an example of applying RAG systems to real-world video content.
Notes
Fully local execution; no cloud APIs required except for optional YouTube downloads.

Best for short-to-medium-length videos for faster processing.

Can be extended to support batch video processing, multi-language transcription, or Streamlit/React interface.

Includes context isolation to prevent mixing data from different videos.

Contact
Shantanu Harkulkar
üìß shantanuhakulkar4@gmail.com
üìû +91 8879528437
üìç Mumbai, Maharashtra, India

Keywords: RAG, LLM, Generative AI, NLP, Whisper, ChromaDB, Mistral, YouTube video analysis

yaml
Copy code

---

If you want, I can also **write a ready-to-use `requirements.txt` and `main.py` template** so anyone cloning your GitHub repo can run the program locally **without extra setup**. This will make it fully interview-ready.  

Do you want me to prepare that too?
